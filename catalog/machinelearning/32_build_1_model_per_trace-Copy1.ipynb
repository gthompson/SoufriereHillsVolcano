{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disciplinary-marks",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "### PREPARE THE CATALOG DataFrame ###\n",
    "SEISAN_DATA = os.path.join( os.getenv('HOME'),'DATA','MVO') # e.g. /home/user/seismo\n",
    "pandaSeisDir = os.path.join(SEISAN_DATA, 'miniseed_c') # e.g. /home/user/seismo/pandaSeis\n",
    "SEISAN_DB = 'MVOE_' # e.g. the seisan database name (e.g. MVOE_)\n",
    "PROJECTDIR = os.path.join(os.getenv('HOME'),'src', 'kitchensinkGT', 'PROJECTS', 'MontserratML') # this dir\n",
    "#csvfile_external = os.path.join(PROJECTDIR, 'MVO_labelled_events.csv')\n",
    "csvfile_external = os.path.join(SEISAN_DATA, 'MachineLearning', SEISAN_DB, 'runAAA', 'MVOE_11_labelled_events.csv')\n",
    "#csvfile_internal = './catalog/MVO_labelled_events_filtered.csv'\n",
    "csvfile_internal = 'catalog/30_MVO_labelled_events_filtered.csv' # has to match that in AAA-master/config/general/newsettings_10.json\n",
    "csvfile_internal = './AAA-master/MONTSERRAT/' + csvfile_internal\n",
    "output_path_cat = csvfile_internal.replace('.csv', '.pd')\n",
    "alltraces_file = '30_alltraceDFs.csv'\n",
    "\n",
    "################################\n",
    "# Machine learning and testing #\n",
    "################################\n",
    "import sys\n",
    "sys.path.insert(0, './AAA-master/automatic_processing')\n",
    "#import tools\n",
    "from config import Config\n",
    "from analyzer import Analyzer\n",
    "\n",
    "# Change if you want your screen to keep quiet\n",
    "# 0 = quiet\n",
    "# 1 = in between\n",
    "# 2 = detailed information\n",
    "verbatim = 1\n",
    "\n",
    "# Init project with configuration file\n",
    "config = Config('./AAA-master/config/general/newsettings_10.json', verbatim=verbatim)\n",
    "config.readAndCheck()  \n",
    "\n",
    "##########################\n",
    "# Variables to loop over #\n",
    "##########################\n",
    "alltraces = pd.read_csv(alltraces_file)\n",
    "\n",
    "traceIDs = ['MV.MBWH..SHZ', 'MV.MBLG..SHZ', 'MV.MBRY..SHZ']\n",
    "minWeights = range(4)\n",
    "classes_to_include = [ ['l', 't'], ['e', 'r'], ['h', 'l', 't'], ['h', 'l', 't', 'r'], ['e', 'h', 'l', 't', 'r'] ]\n",
    "\n",
    "traceIDs = ['MV.MBWH..SHZ', 'MV.MBLG..SHZ', 'MV.MBGB..BHZ', 'MV.MBGH..BHZ']\n",
    "minWeights = range(4)\n",
    "classes_to_include = [ ['r', 'e', 'l', 'h', 't']]\n",
    "combine_re = False\n",
    "\n",
    "traceIDs = ['MV.MBWH..SHZ', 'MV.MBGB..BHN', 'MV.MBGB..BHZ', 'MV.MBGB..BHE', 'MV.MBLG..SHZ', 'MV.MBGH..BHZ', 'MV.MBGH..BHE', 'MV.MBGH..BHN', 'MV.MBGA..BHE', 'MV.MBGA..BHZ', 'MV.MBGA..BHN', 'MV.MBGE..BHE', 'MV.MBGE..BHZ', 'MV.MBGE..BHN', 'MV.MBBE..BHE', 'MV.MBBE..BHZ', 'MV.MBBE..BHN']\n",
    "minWeights = [3]\n",
    "classes_to_include = [ ['r', 'e', 'l', 'h', 't'] ]\n",
    "combine_re = False\n",
    "\n",
    "#############\n",
    "# Functions #\n",
    "#############\n",
    "\n",
    "def cat_filter_traceID(cat, alltraces, traceID):\n",
    "    # subset catalog based on traceID\n",
    "    matchingEvents = alltraces[alltraces['id']==traceID]\n",
    "    for i, row in cat.iterrows():\n",
    "        matching_indices = matchingEvents.index[matchingEvents['filetime']==row['filetime']].tolist()\n",
    "        if len(matching_indices)==1:\n",
    "            pass\n",
    "        else:\n",
    "            cat.drop(i, inplace=True)\n",
    "    N = len(cat.index)\n",
    "    #print('%d events after matching against traceID' % N)\n",
    "    return N\n",
    "\n",
    "def cat_filter_classes(cat, remove_classes):\n",
    "    \"\"\"\n",
    "    for rmclass in remove_classes:\n",
    "        print('Removing %s' % rmclass)\n",
    "        cat = cat[cat['class']!=rmclass]\n",
    "    \"\"\"\n",
    "    cat=cat[cat[\"class\"].isin(remove_classes)]\n",
    "    N = len(cat.index)\n",
    "    #print('%d events after removing classes' % N)\n",
    "    return cat, N\n",
    "\n",
    "def cat_filter_weight(cat, minWeight):\n",
    "    #if minWeight>0:\n",
    "    #    cat = cat[cat['weight']>=minWeight]\n",
    "    cat=cat[cat[\"weight\"].isin(range(minWeight,13))]\n",
    "    N = len(cat.index)\n",
    "    #print('%d events after filtering above %d' % (N, minWeight))\n",
    "    return cat, N\n",
    "\n",
    "def cat_check_numbers(cat, minthresh = 20):\n",
    "    df = cat.copy()\n",
    "    tooSmall = False\n",
    "    lengths = []\n",
    "    for subclass in df['class'].unique():\n",
    "        dfs = df[df['class']==subclass]\n",
    "        N = len(dfs.index)\n",
    "        if N<minthresh:\n",
    "            tooSmall=True\n",
    "        lengths.append(N)\n",
    "    return tooSmall, lengths\n",
    "\n",
    "\n",
    "#######################\n",
    "# Looping starts here #\n",
    "#######################\n",
    "minPerClass = 30\n",
    "counter = 0\n",
    "\n",
    "\n",
    "for combine_re in [False, True]:\n",
    "    results_list = []\n",
    "\n",
    "    for traceID in traceIDs:\n",
    "\n",
    "        # what traceID are we looking for - read_montserrat needs this - should figure out how to write this into the config\n",
    "        fptr = open('./AAA-master/MONTSERRAT/current_traceID.txt','w')\n",
    "        fptr.write(traceID)\n",
    "        fptr.close()\n",
    "\n",
    "        for minWeight in minWeights:\n",
    "            for include_classes in classes_to_include:\n",
    "                # reload cat because we filter it down each time\n",
    "                #cat = pickle.load(open(output_path_cat,'rb'))\n",
    "                cat = pd.read_csv(csvfile_internal)\n",
    "                if combine_re:\n",
    "                    cat.loc[cat['class']=='e', 'class']='r'\n",
    "                    if 'e' in include_classes:\n",
    "                        include_classes.remove('e')\n",
    "\n",
    "                print(cat['class'].value_counts())\n",
    "                print(traceID, include_classes, minWeight)\n",
    "\n",
    "                results_dict = {}\n",
    "                results_dict['traceID'] = traceID\n",
    "                results_dict['classes'] = ','.join(include_classes)\n",
    "                results_dict['minWeight'] = minWeight\n",
    "                results_dict['NtraceID'] = cat_filter_traceID(cat, alltraces, traceID)\n",
    "                cat, N = cat_filter_classes(cat, include_classes) \n",
    "                results_dict['Nclasses'] = N\n",
    "                cat, N = cat_filter_weight(cat, minWeight)\n",
    "                results_dict['Nweight'] = N\n",
    "                tooSmall, lengths = cat_check_numbers(cat, minPerClass)\n",
    "                results_dict['counts'] = str(lengths)\n",
    "\n",
    "                results_dict['acc_mean'] = None\n",
    "                results_dict['acc_std'] = None\n",
    "\n",
    "                if N>=minPerClass*len(include_classes) and not tooSmall:\n",
    "                    #try:\n",
    "                        print(cat.groupby('class').size())\n",
    "                        analyzer = Analyzer(config, verbatim=verbatim, catalog=cat)\n",
    "                        allData, allLabels, acc, allPredictions, allProbabilities = analyzer.learn(config, returnData=True, verbatim=verbatim) # If you want the data\n",
    "                        results_dict['acc_mean'] = np.round(np.mean(acc)*100, 1)\n",
    "                        results_dict['acc_std'] = np.round(np.std(acc)*100, 1)                        \n",
    "                        cat['predicted_class'] = allLabels\n",
    "                        cat['traceID'] = traceID\n",
    "                        for i, classcol in enumerate(sorted(include_classes)):\n",
    "                            colname = 'prob_%s' % classcol\n",
    "                            cat[colname] = allProbabilities[:,i]\n",
    "                        cat.to_csv(csvfile_internal.replace('.csv','_predicted_%d.csv' % counter)) \n",
    "                results_list.append(results_dict)\n",
    "                counter += 1\n",
    "\n",
    "    resultsDF = pd.DataFrame(results_list)\n",
    "    resultsCSV = ''.join(include_classes) + '.csv'\n",
    "    resultsDF.to_csv(resultsCSV)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
