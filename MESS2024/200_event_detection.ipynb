{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c39fe136-f4f0-4e0c-9289-210354fdd6c8",
   "metadata": {},
   "source": [
    "# Event Detection (& Association)\n",
    "The first stage of the monitoring process is to detect anomalous events - short, transient signals of a few seconds to 1-2 minutes. The classic approach (since late 1970s?) has been to use a Short-Term Average (STA) to Long-Term Average (LTA) ratio-based Detector.\n",
    "\n",
    "## Steps:\n",
    "- Run an <b>STA/LTA Detector</b> on each channel of continuous waveform data (to each ObsPy Trace object wthin a Stream object).This yields a set of Trigger (or Detection) ON and OFF times.\n",
    "- Run an <b>Associator</b> on the full set of Triggers. If multiple channels have Trigger ON status at the same time, record as an Event in a Catalog.\n",
    "\n",
    "In ObsPy, both steps can be done using `coincidence_trigger` from the `obspy.signal.trigger` package.\n",
    "\n",
    "## Examples:\n",
    "1. Run an STA/LTA Detector on a single channel of data, on just 1 hour of data.\n",
    "2. Run an STA/LTA Detector and Associator on multiple channels of data (a Stream object), on just 1 hour of data.\n",
    "3. Run an STA/LTA Detection and Associator on multiple channels of data (a Stream object), for multiple days of data.\n",
    "\n",
    "## Dataset\n",
    "For this tutorial, we will use data from the Montserrat Volcano Observatory. On many occasions there was drumbeat seismicity in the days before a major dome collapse of the Soufriere Hills Volcano. We focus on the major dome collapse that occurred on July 12-13th, 2003.\n",
    "\n",
    "The continuous waveform data will come from a \"SeisComP Data Structure\" (https://docs.obspy.org/packages/autogen/obspy.clients.filesystem.sds.html) or \"SDS\" archive. We will explore this first. We can create a client connection to as SDS archive using the Client class from the `obspy.clients.filesystem.sds` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c1882c-98b6-4c07-a801-8e9ebea05de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import general packages\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import obspy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # turn off warnings about Miniseed file encoding (and anything else!)\n",
    "\n",
    "# Import local packages\n",
    "sys.path.append('lib')\n",
    "import vsmTools\n",
    "\n",
    "# Define where SDS archive and EVENTS data are stored\n",
    "DATA_DIR = os.path.join('data')\n",
    "SDS_DIR = os.path.join(DATA_DIR, 'continuous','SDS')\n",
    "EVENTS_DIR = os.path.join(DATA_DIR, 'events')\n",
    "CATALOG_DIR = os.path.join(DATA_DIR,'catalogs')\n",
    "\n",
    "# Create a connection to an SDS archive\n",
    "from obspy.clients.filesystem.sds import Client\n",
    "sdsclient = Client(SDS_DIR)\n",
    "\n",
    "# Show a tree listing of SDS_DIR\n",
    "print('Tree listing of SDS_DIR directory')\n",
    "for line in vsmTools.tree(Path(os.path.join(SDS_DIR, '2003'))):\n",
    "    print(line)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef88b9a-3d57-4fab-957d-2473b79dccd7",
   "metadata": {},
   "source": [
    "Let's look at a helicorder plot of a few hours of data. This will help us choose appropriate STA/LTA parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784ca771-b015-4a1b-8545-d1f18eb73ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = sdsclient.get_waveforms(\"MV\", \"MBWH\", \"\", \"SHZ\", obspy.UTCDateTime(2003, 7, 11), obspy.UTCDateTime(2003, 7, 11, 6, 0, 0))\n",
    "st.plot(type='dayplot', interval=10, vertical_scaling_range=3e3);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60005acf-f904-43d8-8617-26000fa78583",
   "metadata": {},
   "source": [
    "Looking above we can see there were up to 3 events per minute. \n",
    "\n",
    "Seismic and infrasound waves are essentially low-frequency sound waves in the ground and air respectively, so it makes sense to listen to them. ObsPy makes this easy to do:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9053604-8f90-4f8e-846c-82c71100a400",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_audio_file = os.path.join(DATA_DIR, 'MV20030711.wav')\n",
    "st[0].write(wav_audio_file, rescale=True, format='WAV', framerate=6000*2)\n",
    "os.system(f'open {wav_audio_file}') # works on Ubuntu Linux 22.04"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7a9dd7-fe05-444b-a4c8-61598e8a00e3",
   "metadata": {},
   "source": [
    "This is called 'drumbeat seismicity'! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22711f5-066d-479a-965b-07ced30460bc",
   "metadata": {},
   "source": [
    "## Example 1: Detect anomalous signals in 1 hour of data\n",
    "\n",
    "First we load one hour of data from the SDS archive (multiple channels):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7241cb89-650f-47d9-976e-f989e1119472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set start time as \"YYYYJJJT\" where JJJ = (Julian calendar) day of year\n",
    "stime = obspy.UTCDateTime(\"2003193T\")\n",
    "print(stime)\n",
    "seconds = 3600\n",
    "\n",
    "# Get and plot data\n",
    "st = sdsclient.get_waveforms(\"MV\", \"*\", \"*\", \"?HZ\", stime, stime+seconds)\n",
    "st.plot(equal_scale=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dc4c69-b9ae-40d5-a9d7-7f208c4f7dc1",
   "metadata": {},
   "source": [
    "Run STA/LTA Detector on the final Trace object (MV.MBWH..SHZ) in the Stream. We can use index = -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9557af77-01c5-43cd-89b0-c1d53cd01965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detection\n",
    "import numpy as np\n",
    "from obspy.signal.trigger import plot_trigger, classic_sta_lta\n",
    "\n",
    "st2 = st.copy()\n",
    "\n",
    "tr_index = -1\n",
    "\n",
    "Fs = int(np.round(st[tr_index].stats.sampling_rate, 0))\n",
    "print(f\"sampling_rate={Fs}\")\n",
    "\n",
    "corners = 3\n",
    "sta_secs = 2.3\n",
    "lta_secs = 11.5\n",
    "threshON = 2.4\n",
    "threshOFF = 1.2\n",
    "freqmin = 1.5\n",
    "freqmax = 12.0    \n",
    "\n",
    "st2.filter('bandpass', freqmin=freqmin, freqmax=freqmax, corners=corners)\n",
    "sta_samples = int(sta_secs * Fs)\n",
    "lta_samples = int(lta_secs * Fs)\n",
    "\n",
    "#cft = recursive_sta_lta(st2[tr_index].data, sta_samples, lta_samples )\n",
    "#threshOFF = 1.2\n",
    "cft = classic_sta_lta(st2[tr_index].data, sta_samples, lta_samples )\n",
    "\n",
    "plot_trigger(st2[tr_index], cft, threshON, threshOFF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a11bcb3-600f-4af8-b2b6-82e91622b87f",
   "metadata": {},
   "source": [
    "The top plot shows Trigger ON (red lines) and Trigger OFF (blue lines) times. The bottom line shows the STA:LTA ratio, where \"Trigger ON\" (red) corresponds to 2.4, and \"Trigger OFF\" corresponds to 0.5).\n",
    "\n",
    "## Example 2: Run STA/LTA on all (good) channels\n",
    "Next we run `coincidence_trigger` on the whole Stream object, choosing the same 'Classic STA/LTA\" method as before, and the same input parameters. Since we have 6 'good' channels (Trace objects), we will require at least 3 stations to trigger ON at the same time before a network trigger is declared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5eacd1-3738-482a-b0ac-a3ea9cb309e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from obspy.signal.trigger import coincidence_trigger\n",
    "from pprint import pprint\n",
    "\n",
    "threshStations = 3 # minimum number of channels that must reach trigger ON at the same time before a network trigger is declared.\n",
    "max_secs = 120.0 # no trigger will remain on for longer than 2 minutes\n",
    "\n",
    "staltamethod = 'classicstalta'\n",
    "trig = coincidence_trigger(staltamethod, threshON , threshOFF, st2, threshStations, sta=sta_secs, lta=lta_secs, max_trigger_length=max_secs, details=True)\n",
    "\n",
    "for i, this_trig in enumerate(trig):\n",
    "    print('\\n',f'Event {i}')\n",
    "    pprint(this_trig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ec1518-4d62-4886-be27-b9e5ab3cd9d7",
   "metadata": {},
   "source": [
    "93 network triggers were declared within that one hour time window. \n",
    "\n",
    "The trig variable is a list of dictionaries. Each dictionary describes a single network trigger, and contains fields including 'time' which has the UTCDateTime at which the network trigger began, 'coincidence_sum' stores the number of channels that simultaenously triggered, and 'trace_ids' tells us the specific channels that triggered. \n",
    "\n",
    "We will assume that each trigger a volcano-seismic event, i.e. a transient (short-lived) anomalous signal. ObsPy includes classes (object-oriented blueprints of new data types useful for Seismology) for storing each `Event` within a `Catalog`. The structure of an ObsPy `Event` object is based on the QuakeML format, and though this isn't great match for volcano-seismic events, we will create an `Event` object for each network trigger. However, rather than use an ObsPy `Catalog` object directly, we have created a new class `VolcanoSeismicCatalog` which inherits `Catalog` and expands on it. This should all be invisible to you - it happens in the `tools` package, imported above. All we have to do is call `tools.triggers2catalog` and it returns a `VolcanoSeismicCatalog` object.\n",
    "\n",
    "It will also segment out a window of data for each Event beginning pretrig seconds before the trigger ON time and ending posttrig seconds after the trigger OFF time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fc0e56-e260-487a-b14e-5f7879f9f226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pretrig = 10 #lta_secs * 2\n",
    "posttrig = 20 # lta_secs * 2   \n",
    "catalogObj = vsmTools.triggers2catalog(trig, staltamethod, threshON, threshOFF, \\\n",
    "                                        sta_secs, lta_secs, max_secs, stream=st2, pretrig=pretrig, posttrig=posttrig )\n",
    "    \n",
    "print(catalogObj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38eb3f44-961e-4bc6-a6c1-82aacfff61d4",
   "metadata": {},
   "source": [
    "By expanding on the ObsPy `Catalog` class, we have added several new methods, some of which we will use. For example, we can make plots of event rate and energy release rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fcbf12-fd0a-4e20-a609-d4ed265da38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogObj.plot_eventrate(binsize=pd.Timedelta(minutes=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae5d0aa-0c9b-40df-93b0-295cbd945b1f",
   "metadata": {},
   "source": [
    "So we have the basic ability to detect events. Unlike the examples in the presentation which are chosen for clarity and having an excellent signal to noise ratio, the events shown in the plots above are not really obvious at all. This is quite typical. Indeed, in this case we are trying to detect events amongst a background of heightened unrest - earthquake swarms that became drumbeat-like and merge into a continuous tremor. It is important to note that STA/LTA algorithms often break down when volcano-seismicity is at its highest!\n",
    "\n",
    "Anyway, let us now try to run this on several days on data and see how many events we can detect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bc9237-0a7a-4bee-98e3-c5f4e898b62d",
   "metadata": {},
   "source": [
    "## Example 3: Run coincidence trigger on multiple days, and concatenate catalogs for each day into one\n",
    "\n",
    "(This takes about 1 minute to run on my office Linux PC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e30b08e-9346-465b-8d88-7c67a8c7f06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "threshStations = 3 # minimum number of stations that must trigger for a coincidence trigger to occur\n",
    "max_secs = 120.0\n",
    "staltamethod = 'classicstalta' \n",
    "corners = 3\n",
    "sta_secs = 2.3\n",
    "lta_secs = 11.5\n",
    "threshON = 2.4\n",
    "threshOFF = 1.2\n",
    "freqmin = 1.5\n",
    "freqmax = 12.0  \n",
    "catalog_all = None\n",
    "\n",
    "for jday in range(190, 197, 1): # Loop over days in SDS archive\n",
    "    for hour in range(24):\n",
    "        # set start time & show counting through days and hours\n",
    "        stime = obspy.UTCDateTime(f\"2003{jday}T\") + hour * 3600\n",
    "        if hour==0:\n",
    "            print(stime.strftime('%Y/%m/%d %H'), end=' ')\n",
    "        elif hour==23:\n",
    "            print(stime.hour)\n",
    "        else:\n",
    "            print(stime.hour, end=' ')\n",
    "    \n",
    "        # Get and plot data\n",
    "        st = sdsclient.get_waveforms(\"MV\", \"*\", \"*\", \"?HZ\", stime, stime+3600)\n",
    "        if len(st)==0:\n",
    "            continue\n",
    "    \n",
    "        # remove bad channels, including those with insufficient samples\n",
    "        for tr in st:\n",
    "            if tr.stats.npts < max_secs * tr.stats.sampling_rate:\n",
    "                st.remove(tr)\n",
    "\n",
    "        if len(st)==0:\n",
    "            continue\n",
    "    \n",
    "        # bandpass filter\n",
    "        st.filter('bandpass', freqmin=freqmin, freqmax=freqmax, corners=corners)\n",
    "    \n",
    "        # run coincidence trigger for this day\n",
    "        trig = coincidence_trigger(staltamethod, threshON , threshOFF, st, threshStations, sta=sta_secs, lta=lta_secs, max_trigger_length=max_secs, details=True)\n",
    "        if len(trig)==0:\n",
    "            continue\n",
    "\n",
    "        # create catalog object\n",
    "        catalogObj = vsmTools.triggers2catalog(trig, staltamethod, threshON, threshOFF, \\\n",
    "                                                      sta_secs, lta_secs, max_secs, stream=st, pretrig=pretrig, posttrig=posttrig )\n",
    "    \n",
    "        # save the event streams to miniseed, and then erase them to free up memory\n",
    "        catalogObj.write_events(outdir=EVENTS_DIR)\n",
    "        catalogObj.streams = []\n",
    "    \n",
    "        # concatenate catalogs\n",
    "        if catalog_all:\n",
    "            catalog_all.concat(catalogObj)\n",
    "        else:\n",
    "            catalog_all = catalogObj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e00cc4-8853-431d-80ab-88ca9c943dfa",
   "metadata": {},
   "source": [
    "If we print this 1-week-long catalog, we see it has ~4300 events. Over 600 events per day on average!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e82b2cb-0efd-4aac-81ad-ae1999c0a879",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(catalog_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce422e3-edf0-460e-8ee1-e8c723314587",
   "metadata": {},
   "source": [
    "Since the events are not located, the latitude, longitude, and depth are set to None.\n",
    "\n",
    "The magnitude (column that starts with 0.44) is not calibrated and is based on the log10(peak_amplitude) of each Trace. Since the Traces are raw (not instrument corrected), the peak amplitude is in Counts. A bit more on this below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e75dc7f-fd42-4b3f-9c3e-22742f662445",
   "metadata": {},
   "source": [
    "We can print a single event too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f55880-0ad9-4879-aed6-613ac6a291e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(catalog_all.events[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3be18b-48b8-40b3-9bac-ff07b9524389",
   "metadata": {},
   "source": [
    "And view the amplitude information that came from the coincidence trigger: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5420a7cb-e153-43e5-a34c-bbcd5857b36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in catalog_all.events[0].amplitudes: # a is an ObsPy Amplitude object\n",
    "    print(f\"id={a['waveform_id'].id}, amp={a['generic_amplitude']:.1f}, snr={a['snr']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d39299-4170-42b9-96db-f4104fc6acd9",
   "metadata": {},
   "source": [
    "We also have a very first-order magnitude, which is uncalibrated (we'll improve on this soon), but somewhat useful as a real-time relative size indicator. It is computed as the base-10 logarithm of the median amplitude of an event across the set of triggered stations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e078ea-6608-4640-afa4-bf5ee7fc5e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sm in catalog_all.events[0].station_magnitudes: # a is an ObsPy Amplitude object\n",
    "    print(f\"id={sm['waveform_id'].id}, mag={sm['mag']:.1f}\")\n",
    "for m in catalog_all.events[0].magnitudes: # a is an ObsPy Amplitude object\n",
    "    print(f\"magnitude={m['mag']:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a72e85b-65e1-47d2-b341-e26666e8ee9c",
   "metadata": {},
   "source": [
    "### Convert catalog object to a pandas DataFrame\n",
    "While it is great to make use of the ObsPy Catalog class(which we inherited and expanded into a VolcanoSeismicCatalog class), so we can leverage its methods like exporting to QuakeML, it is also much easier to work with a pandas dataframe.\n",
    "\n",
    "Anyone not familiar with pandas dataframes? They are like Excel spreadsheets (and can be imported from and exported to Excel spreadsheets or CSV files), but are far more powerful!\n",
    "\n",
    "Anyway, we convert a VolcanoSeismicCatalog object to a dataframe like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fad6fa3-292e-4821-8849-86c4bd3d1d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "catDF = catalog_all.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22ae0b7-e277-4279-8db8-8c58763c2378",
   "metadata": {},
   "source": [
    "And we can view it like any other pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0952724-15f4-4921-9774-4f2e0b451e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(catDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1832bdb-90c0-43d9-9dab-feb6b6189298",
   "metadata": {},
   "source": [
    "As you can see, so far we have no latitude/longitude/depth coordinates, and no classifications. The energy column here is based on the magnitude - and we'll improve on this in later notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a4a7dc-8758-43cc-a335-ed55b0688b74",
   "metadata": {},
   "source": [
    "And pandas dataframes with a datetime index/column are easy to plot, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abcfcae-f1fc-4dc2-ab47-904ecec0d7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "catDF.plot.scatter(x='datetime', y='duration', s=0.1, rot=90, xlabel='Date', ylabel='Trigger duration (s)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01df8226-3404-4b45-8756-dcb68f072b3d",
   "metadata": {},
   "source": [
    "Recall that 120s was our max trigger length. A few of the events early on are 120s long. But the bulk of the events are much shorter. Indeed, we can change the y-limits and see: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f70d09-79f4-4893-a855-d814a4ed5bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = catDF.plot.scatter(x='datetime', y='duration', s=0.1, rot=90, xlabel='Date', ylabel='Trigger duration (s)')\n",
    "ax.set_ylim([0,20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb52fb7-fb98-4fdd-b516-88fade0d3c1f",
   "metadata": {},
   "source": [
    "This means we did a good job of capturing short transient events, but missed longer signals. This is a common trade-off with STA/LTA methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b07a45c-2f18-4813-b9fd-4ba850af0efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = catDF.plot.scatter(x='datetime', y='magnitude', s=0.1, rot=90, xlabel='Date', ylabel='Uncalibrated Magnitude')\n",
    "ax.set_ylim([0, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f827b6b9-d0ab-4e40-9e69-b2552afaa805",
   "metadata": {},
   "source": [
    "We can observe that the magnitude of the detected events increases significantly from about 2003/07/12 08:00 until about 2003/07/13 02:00. Let's see what is happening in the raw data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408ebc2f-645d-493d-8b6a-9198652ddbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "st2 = sdsclient.get_waveforms(\"MV\", \"MBWH\", \"\", \"SHZ\", obspy.UTCDateTime(2003, 7, 12, 8, 0, 0), obspy.UTCDateTime(2003, 7, 13, 2, 0, 0))\n",
    "st2.plot(type='dayplot', interval=10, vertical_scaling_range=8e3);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8827827-0279-432d-aac2-99f91e25ab91",
   "metadata": {},
   "source": [
    "We observe that the background level of seismicity is increasing. This is a progressive dome collapse that began with intense rainfall leading to lahars and individual pyroclastic flows that eventually become near-continuous and larger until most of the dome is gone. such rainfall-triggered dome collapses were common in Montserrat, and this underlines the importance of real-time rain gauge data. However, this was very much a dome that was ready collapse, since drumbeat seismicity usually resulted in dome collapses after 2-3 days. What isn't clear from this plot is whether the drumbeats merged into tremor as the drumbeats become drowned out by tremor-like lahar signals and much larger pyroclastic flow signals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fadff4-5ea2-4eac-af3f-1183d7af79d3",
   "metadata": {},
   "source": [
    "Let's save the whole catalog so we can load it in later notebooks. This creates an QuakeML (extension \".xml\") file of the whole catalog, plus a Pickle file (extension \".pkl\") containing variables that QuakeML cannot handle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59541fcf-1850-486c-b597-3cfd5ed689ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_all.save(CATALOG_DIR,'catalog_MV_20030712')\n",
    "os.listdir(CATALOG_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22395ace-c122-4072-8e86-a9fb1a41dfec",
   "metadata": {},
   "source": [
    "Finally for the tutorial, we will visualize the catalog, using a binsize of 1 hour, and then 10 minutes. Pandas dataframes are easy to resample!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b96d86-763c-4ebc-8304-4d100ccefaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "catalog_all.plot_eventrate(binsize=pd.Timedelta(hours=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a302e05-44b8-4043-8154-1ad851993ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_all.plot_eventrate(binsize=pd.Timedelta(minutes=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cf0a68-6b4e-4450-b1cf-a1d45feeec24",
   "metadata": {},
   "source": [
    "The 10-minute binsize plot gives us a better indication of the rapid drop in detected event counts. We can zoom in on this with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4084dcd5-f55f-42cb-aa0f-e71f5403ab07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "time_limits = [datetime.datetime(2003, 7, 12, 0, 0, 0), datetime.datetime(2003, 7, 14, 12, 0, 0)]\n",
    "axs = catalog_all.plot_eventrate(binsize=pd.Timedelta(minutes=10), time_limits=time_limits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5728f6b-f1cb-4db0-9b13-099271eb06b0",
   "metadata": {},
   "source": [
    "How would you interpret the drop in event rate if you didn't have the magnitude or energy release rate plot too? Probably as a drop in overall seismicity!\n",
    "\n",
    "### Observations:\n",
    "- Most of the detected events occur in a 56-hour period from 2003/07/10 00:00, peaking around 120 events per hour.\n",
    "- There is an abrupt drop in triggered event rate beginning at 2003/07/12 08:00\n",
    "- The magnitude of events rises sharply from about 2003/07/12 12:00 through to 2003/07/13 02:00. Only the largest events trigger because of the high background \"tremor\" from lahars and pyroclastic flows.\n",
    "- The energy release rate (from cumulative magnitude) peaks around the start of 2003/07/13 for about 4 hours.\n",
    "\n",
    "### Key points:\n",
    "- <b>Catalogs are flawed. They often show no seismicity when seismicity and hazards are peaking!</b>\n",
    "- <b>Event rate and Energy release rates give us different information!</b>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
